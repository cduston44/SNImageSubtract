{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59934be",
   "metadata": {},
   "source": [
    "Ok so this here sheet is going to be the full image subtraction routine for SN photometry. Note that it is being written to be applied to a single filter at a time, for simplicity!\n",
    "\n",
    "Note that following the original script from GROWTH 2020, this script will remove the processed directories when you run it, so careful!\n",
    "\n",
    "This version does not require any alginment beforehand, but should be plate-solved (not entirely sure AIJ routine is good enough, might need to write another to handle that - there is one in the rotation sheet). **Without the plate solve, this script will throw you strange errors!**\n",
    "\n",
    "**The source directory should contain just the source images**, and the configuration directory has a bunch of configuration files that this script needs. The data directory will be created by this code as the sources images + conifguration files, and the processed directory will be....the processed directory! This is awkward, but can't really figure out how to make it work otherwise...the out directory contains some random files that are produced in the process, as well as the final difference images.\n",
    "\n",
    "Note you will have to move the processed files out of the processed/out/ directory into long-term storage of some kind.\n",
    "\n",
    "Revision History:\n",
    "<1.2: ?!?!?!\n",
    "\n",
    "1.2: Does everything I hope?\n",
    "    koy: Did a new WCS using the worksheet to fix alignment problems. **This worked!!**\n",
    "    \n",
    "\n",
    "20240813: Doing AT2023sse. This basically failed, you really can't see the source at all. I did modify the pfsex_config file to shrink the size of the PSF, and that still did not work. The subtracted images have multiple problems, including offsets, so perhaps I should try my plate solve method before giving up? Nope, better WCS didn't help :-/ Added a filter on the catalogs that calculate the normalization, to kill all flags over 4. Then give up!!! The filtered worked, but did not impact the resulting subtraction, so we are giving up!\n",
    "\n",
    "CORRECTION: I've convinced myself that it's the size of the windowed region that borking things up, so just shrinking it until there are no saturated sources in it at all. That's just subtraction, so it shouldn't impact things toooooo much? Yup, it ran, and \"the problem\" was fixed, ugh.\n",
    "\n",
    "1.3 Got a whole bunch more options now to deal with see - make sure the defaults are set below before you run the thing!\n",
    "\n",
    "20240909: Doing AT2023umf. Making images smaller...\n",
    "\n",
    "20240912: Doing AT2023pnt. AIJ WCS didn't really kill all the galaxy signal, so trying my Plate Solve routine. Galaxy is still present in the subtraction. Shrinking, debugging...There is a psf region hitting into the subtraction region, so shrinking the subtraction region. Might have fixed some, but not many. Remove outliers in flux ratio...removing background masking...reverting, that was probably slightly worse. Trying catalog filter before normalization. No help, but negative flux ratios!? Fixed my filter, STILL not subtracting the galaxy! Shrink window more? Ok, \"last\" thing to try is do the background subtraction before resampling...nope! Masking saturated starts from the beginning? (another possible solution - close saturated stars to target are getting PSF smeared in to the target! Ok, finally I have fixed the PSF problem by masking the saturated stars...I should implement that for all of the images, at the very beginning!\n",
    "\n",
    "20241007: Doing AT2023yfh, with the new saturated star mask, right before PSF subtract (so background subtract is unaltered). Still looks pretty bad! turn up level, so more stars are masked? Well, it actually looks like the background subtraction is not working that well - lots of excess still there. Remove masking? Yes, that seemed to do a better job of removing the large scale gradients. Filter out the bright images left after subtraction...no help, try the outlier removal. Also no real help. Back to beginning, make entire image smaller? Ahh ok that actually kinda looks right! At least the source is remaining after removing....something! For B, subtraction somewhat less good, see if I can help. Saturation subtraction does not look right! Removing that and sticking with it.\n",
    "\n",
    "20250103: Redoing SN2023cyx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3762d",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "There are a number of external codes that this process uses. I've managed to get them up and running on Pop_OS, but not Slackware, so these instructions are not a guarantee!\n",
    "\n",
    "(there are other things that need to be install via pip, like photutils,image_registration,astroalign....)\n",
    "\n",
    "SWarp https://www.astromatic.net/software, in apt but that version doesn't seem to do anything...trying from source. That lead me down the autogen rabbithole. Install autoconf,libtool via apt, then ./autogen.sh. Now configure and make.\n",
    "\n",
    "SExtractor https://www.astromatic.net/software (this appears to be in apt...yeah it's a name change, it's source-extractor now!)\n",
    "\n",
    "PSFex https://www.astromatic.net/software (in apt!)\n",
    "\n",
    "ds9 http://ds9.si.edu/site/Home.html (seems to just be a binary, so putting it in /usr/local/bin!) (this is not actually used in this sheet, just including it for full information...it pops out the graphics in a new window and allows for interactions, so might be helpful later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c0efd",
   "metadata": {},
   "source": [
    "# \"Instructions\"\n",
    "\n",
    "Just to be very clear, things that need to be done to make this sheet workable. Do this stuff FIRST - if you, for example, copy the files in after one of the cells that tried to copy them into the right place, you will fail!\n",
    "\n",
    "1. Plate solve stacked images - AIJ first.\n",
    "2. Copy images for a single filter into the \"source\" directory.\n",
    "3. Change the source position in the cell below.\n",
    "4. Change everything back to default before running for the first time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018ff192",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some parameters first.\n",
    "\n",
    "# Images will be resampled to this size (might have x,y switched!)\n",
    "# Ok, these seem not to impact the resampled files, which are the only thing we use.\n",
    "\n",
    "# images generally need to be cropped down below their smallest dimension. Look for black edges and bad\n",
    "# PSFs to be signs that this needs to be smaller.\n",
    "SWARPSIZEX=400 #Default 1800\n",
    "SWARPSIZEY=400 #Deault 900\n",
    "\n",
    "# bg changes sometimes needed for smaller images\n",
    "bg_size=30 # for smoothing the background estimator (default 50)\n",
    "filter_size=3 # size for median filter of background estimator (default 3)\n",
    "rad=10 #pixels for source subtract in the background, default 10 (tests with AIJ indicate 6 is source size...)\n",
    "\n",
    "# source position (needed for the windowed subtraction scheme)\n",
    "RA=206.275465063\n",
    "Dec=3.80370200338\n",
    "window=30 # size of windowed region (50 default) Be very careful, we need like 10-15 pixels of RADIUS for the photometry!\n",
    "\n",
    "# Mask sources in background estimation\n",
    "bg_mask_flag=1 # 1 for masked, 0 for unmasked!\n",
    "\n",
    "# Now a flag for masking out saturated sources *completely* in the image. This was originally done to fix\n",
    "# pnt, but it seems to be a fix for the bad PSF problem in previous images. It should maybe be the default!\n",
    "# Note there is also a saturation level which can be set in the that block, lower down.\n",
    "sat_mask_flag=1 # default 0\n",
    "pixmask=1 # connected pixels needed to mask, so MORE means LESS masking\n",
    "SAT_FRAC=0.80 # Above this saturation fraction, mask the sources\n",
    "\n",
    "# We wrote a routine to filter the catalogs before normalization to remove flagged sources (generally saturated or\n",
    "# other bad things) but that did not seem to improve the situation here (sse), so we are giving up! Change this flag\n",
    "#if you like, but no evidence that it actually helps!\n",
    "filter_cat_flag=1 # default 0\n",
    "# in the outlier elimination, also - reduced the spread over the normalization, but didn't actually improve\n",
    "# the resulting images! arg! This gives a 3 sigma filter on the flux ratios.\n",
    "eliminate_outliers_flag=1 # default 0\n",
    "show_images_flag=1 # default 0, just shows more information during the subtraction step.\n",
    "extract_thresh=5 # Threshhold for detection above sigma - default 5\n",
    "\n",
    "# This flag is going to implement the background subtraction routine BEFORE the resampling step.\n",
    "# This is not the original behavior of the GROWTH workshop, but we are trying to deal with pnt,\n",
    "# and the subtraction seems to be very inconsistent\n",
    "bg_subtract_pre_resamp_flag=0 # default 0 Didn't help the first time I tried it!\n",
    "\n",
    "#sometimes we need to make this smaller, when the noise is such that the signal in the PSF doesn't dominant\n",
    "# comes as a normalization warning.\n",
    "size_of_psf=31 #default 59 - must be odd!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaae65d",
   "metadata": {},
   "source": [
    "First import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbfc541c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os  #Call commands from outside Python\n",
    "import numpy as np\n",
    "\n",
    "# Running external programs\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "from astropy.coordinates import SkyCoord # For the catalog analysis during normalization\n",
    "import astropy.units as u # for coordinate units\n",
    "\n",
    "from astropy.io import fits #FITS files handling\n",
    "from astropy.io import ascii  #Read/write ascii files\n",
    "from astropy.wcs import WCS # for the cropping routine\n",
    "\n",
    "\n",
    "# Background subtraction\n",
    "import photutils\n",
    "#from photutils.detection import DAOStarFinder\n",
    "from photutils.segmentation import detect_threshold, detect_sources\n",
    "#from photutils import Background2D, MedianBackground # old version\n",
    "from photutils.background import Background2D, MedianBackground \n",
    "from astropy.stats import sigma_clipped_stats, SigmaClip # statistics\n",
    "from photutils.utils import circular_footprint\n",
    "\n",
    "# Image registration and shifting\n",
    "from image_registration import chi2_shift\n",
    "from image_registration.fft_tools import shift\n",
    "import scipy\n",
    "from scipy import ndimage, misc\n",
    "from astropy.table import Table # for catalog manipulation\n",
    "\n",
    "# Useful to smooth the images with a Gaussian kernel before the subtraction\n",
    "from scipy.signal import convolve as scipy_convolve\n",
    "from astropy.convolution import convolve, convolve_fft # for saturation star masks during convolution.\n",
    "\n",
    "# For rotation\n",
    "import astroalign as aa\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Math help\n",
    "import math\n",
    "\n",
    "# Aperture photometry at the end\n",
    "from photutils.aperture import CircularAperture, ApertureStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a83eda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You start from the directory: /media/cduston/MCARG/AnalyzedSources/SN2023cyx/image_subtraction/OIS_test\n",
      "Error, config directory does not exist!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_187559/2786937433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error, config directory does not exist!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# copy sources into data directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "# Set directory structure\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(\"You start from the directory:\", cwd)\n",
    "\n",
    "source_dir = os.path.join(cwd, 'source') # All the stacked, rotated files should be in here\n",
    "data_dir = os.path.join(cwd, 'data') # empty, but will get rewritten.\n",
    "if os.path.isdir(data_dir): # checks and removes if it exists. \n",
    "    shutil.rmtree(data_dir)\n",
    "os.mkdir(data_dir)\n",
    "proc_dir = os.path.join(cwd, 'processed') # all processes fits files.\n",
    "out_dir = os.path.join(proc_dir, 'out') # output files are here\n",
    "if os.path.isdir(proc_dir): # checks and removes if it exists. \n",
    "    shutil.rmtree(proc_dir)\n",
    "os.mkdir(proc_dir)\n",
    "config_dir = os.path.join(cwd, 'config') # various configuration files, should exist!\n",
    "if os.path.exists(config_dir)==False:\n",
    "    print(\"Error, config directory does not exist!\")\n",
    "    raise\n",
    "\n",
    "for f in os.listdir(source_dir): # copy sources into data directory.\n",
    "    shutil.copy2(os.path.join(source_dir, f), os.path.join(data_dir,f))\n",
    "for f in os.listdir(config_dir): # copy configuration into data directory.\n",
    "    shutil.copy2(os.path.join(config_dir, f), data_dir)\n",
    "for f in os.listdir(data_dir): # copy data into processed and change to it.\n",
    "    shutil.copy2(os.path.join(data_dir, f), proc_dir)\n",
    "\n",
    "os.chdir(proc_dir)\n",
    "print(\"You are working in the image_subtraction/processed/ directory: \")\n",
    "print(\"Full path:\", proc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee6ff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rot_name_array=[]\n",
    "for f in os.listdir(source_dir): # create list of raw images\n",
    "    rot_name_array.append(f)\n",
    "    \n",
    "rot_name_array.sort() # sorted by name so that discovery is first.\n",
    "rot_name_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bab47",
   "metadata": {},
   "source": [
    "## Subtraction Routine\n",
    "\n",
    "This part comes from the GROWTH 2020 school, with my own modifications and dealing with some recent changes to the code. We'll start with a dependency check:\n",
    "\n",
    "(for now skipping all reference to ds9, since that doesn't actually seem that useful atm.)\n",
    "\n",
    "---\n",
    "\n",
    ">This box is for FAILED Slackware installation!\n",
    "\n",
    ">installing these things was kind of a nightmare...these instructions are for Slackware...\n",
    "\n",
    ">Swarp: https://www.astromatic.net/software download source. run autogen? ./configure, make, su, make install...\n",
    "\n",
    ">SExtractor https://www.astromatic.net/software source....will need ATLAS and FFTW. trying autogen. ATLAS not in 15.0 yet! Guess I'll switch to System76 and try old Slackbuilds. Ugh ATLAS does not seem to build on Slack 15.0 (email thread). Trying the intel MKL approach...following wget instructions from https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit-download.html?operatingsystem=linux&distributions=offline. Compatibility warnings, proceeding...isntall location /opt/intel/oneapi. Failing lots of prereqs. All gui, so just pushing forward. Skip eclipse. Need g++, but I actually have that installed, it just can't find it. continuing...says done. trying to configure with --enable-mkl, but cannot find c compilter. \n",
    "\n",
    ">ATLAS alone does not seem to build either on Slackware.....bust!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0795c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def test_dependency(dep, alternate_name=None):\n",
    "    \"\"\"\n",
    "    Test external dependency by trying to run it as a subprocess\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.check_output(dep, stderr=subprocess.PIPE, shell=True)\n",
    "        print(\"%s is installed properly as %s. OK\" % (dep, dep))\n",
    "        return 1\n",
    "    except subprocess.CalledProcessError:\n",
    "        try:\n",
    "            subprocess.check_output(alternate_name, stderr=subprocess.PIPE, shell=True)\n",
    "            print(\"%s is installed properly as %s. OK\" % (dep, alternate_name))\n",
    "            return 1\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"===%s/%s IS NOT YET INSTALLED PROPERLY===\" % (dep, alternate_name))\n",
    "            return 0\n",
    "\n",
    "#dependencies = [('sextractor', 'sex'), ('SWarp', 'swarp'), ('psfex', 'PSFEx'), ('ds9', 'DS9')]\n",
    "#dependencies = [('sextractor', 'source-extractor'), ('SWarp', 'swarp'), ('psfex', 'PSFEx'), ('ds9', 'DS9')]\n",
    "dependencies = [('sextractor', 'source-extractor'), ('SWarp', 'swarp'), ('psfex', 'PSFEx')]\n",
    "i = 0\n",
    "for dep_name1, dep_name2 in dependencies:\n",
    "    i += test_dependency(dep_name1, dep_name2)\n",
    "print(\"%i out of %i external dependencies installed properly.\\n\" % (i, len(dependencies)))\n",
    "if i != len(dependencies):\n",
    "    print(\"Please correctly install these programs before continuing by following the instructions in README.md.\")\n",
    "else:\n",
    "    print(\"You are ready to continue.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30da90bc",
   "metadata": {},
   "source": [
    "## Align the images\n",
    "\n",
    "Use the AstrOmatic Swarp package to align the images.  Swarp relies on the astrometric information of the image (in other words, on the sky coordinates), therefore both the science and reference images must be astrometrically calibrated (for example, using the AstrOmatic SCAMP package).  In this module we assume that the input images are already calibrated.\n",
    "\n",
    "We are going to pass the entire list to swarp at once, methinks. Create the swarp filelist dynamically, to reduce the kinds of errors we will make in the future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d70388",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rot_image_array=[]\n",
    "swarp_filelist_name=config_dir+\"/\"+'swarp_filelist.txt'\n",
    "swarp_filelist = open(swarp_filelist_name, 'w')\n",
    "for f in rot_name_array: \n",
    "    #rot_image_array.append(f)\n",
    "    print(f)\n",
    "    swarp_filelist.write(str(f) + \"\\n\")\n",
    "swarp_filelist.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc96cf",
   "metadata": {},
   "source": [
    "Note in this next command is what crops the images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638fe8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Swarp command\n",
    "# had to change capitalization of SWarp!\n",
    "try:\n",
    "    #command = \"swarp %s %s -c %s -SUBTRACT_BACK N -RESAMPLE Y -RESAMPLE_DIR . -COMBINE N -IMAGE_SIZE 1800,900\" % (sci_image_name, ref_image_name, os.path.join(config_dir, 'config.swarp'))\n",
    "    if bg_subtract_pre_resamp_flag==0:\n",
    "        command = \"swarp @%s -c %s -SUBTRACT_BACK N -RESAMPLE Y -RESAMPLE_DIR . -COMBINE N -IMAGE_SIZE %s,%s\" % (swarp_filelist_name, os.path.join(config_dir, 'config.swarp'),SWARPSIZEX,SWARPSIZEY)\n",
    "    elif bg_subtract_pre_resamp_flag==1:\n",
    "        print(\"Subtracting Background Before resampling...\\n\")\n",
    "        command = \"swarp @%s -c %s -SUBTRACT_BACK Y -RESAMPLE Y -RESAMPLE_DIR . -COMBINE N -IMAGE_SIZE %s,%s\" % (swarp_filelist_name, os.path.join(config_dir, 'config.swarp'),SWARPSIZEX,SWARPSIZEY)\n",
    "    print('Executing command: %s' % command)\n",
    "    rval = subprocess.run(command.split(), check=True)\n",
    "    print('Success!')\n",
    "except subprocess.CalledProcessError as err:\n",
    "    print('Could not run SWarp with exit error %s'%err)\n",
    "\n",
    "# Gotta fix the names of the aligned images\n",
    "swarp_filelist = open(swarp_filelist_name, 'r')\n",
    "align_image_array=[]\n",
    "for line in swarp_filelist:\n",
    "    #print(line)\n",
    "    align_image_array.append(line.strip().replace(\".fits\", \".resamp.fits\").replace('data','processed'))\n",
    "#sci_image_aligned_name = sci_image_name.replace(\".fits\", \".resamp.fits\").replace('data','processed')\n",
    "#ref_image_aligned_name = ref_image_name.replace(\".fits\", \".resamp.fits\").replace('data','processed')\n",
    "swarp_filelist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed9a20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "align_image_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67502f19",
   "metadata": {},
   "source": [
    "Can use AstroImageJ to verify this basically looks right. - putting in a image check step here, since the next step sometimes gets borked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c4370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(align_image_array)):\n",
    "    image = fits.open(align_image_array[i])\n",
    "    mean, median, std = sigma_clipped_stats(image[0].data)\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # set the scale of the image based on its statistics\n",
    "    # Name looks totally wrong!\n",
    "    plt.imshow(image[0].data, vmin=median-2*std, vmax=median+2*std)\n",
    "    plt.colorbar(shrink = 0.4)\n",
    "    plt.title('Resampled image: '+align_image_array[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b207d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Background Subtraction\n",
    "- Mask sources in images\n",
    "- Use 3 sigma clipping to filter data and accurately measure the backgorund\n",
    "- Then split image into 300x300 pixel boxes and apply 2x2 median filter\n",
    "\n",
    "That was the original. We are going to (for the first image)\n",
    "\n",
    "1) look at the scalar background (for no good reason!)\n",
    "2) look at the 2D background with no mask\n",
    "3) look at the 2D background a mask\n",
    "\n",
    "And then assume (3) is best and go ahead with subtracting them all!\n",
    "\n",
    "Note that the final background subtraction is skipped if the flag indicating that it should already have\n",
    "happened during the resampling step is set to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c715f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's loop through the images and check the essential statistics on each background.\n",
    "\n",
    "sigma_clip = SigmaClip(sigma=3.0, maxiters=20) # parameters for estimation\n",
    "footprint = circular_footprint(radius=rad) #size of source subtraction?\n",
    "print(\"Mean, Median, and Standard Deviation for the background in each image\\n\\n\")\n",
    "for i in range(len(align_image_array)):\n",
    "    image = fits.open(align_image_array[i])\n",
    "    # Eventually do this for each image in the array.\n",
    "    #image = sci_image_aligned # remove for multi image!\n",
    "    #image = fits.open(align_image_array[0])\n",
    "    hdr = image[0].header #save fits header\n",
    "    threshold = detect_threshold(image[0].data, nsigma=2.0, sigma_clip=sigma_clip)\n",
    "    segment_img = detect_sources(image[0].data, threshold, npixels=10)\n",
    "    \n",
    "    # This next line fails sometimes, so let's put in some debugging\n",
    "    #plt.imshow(segment_img, origin='lower', cmap=segment_img.cmap,interpolation='nearest')\n",
    "    #plt.colorbar()\n",
    "    #plt.show()\n",
    "    \n",
    "    mask = segment_img.make_source_mask(footprint=footprint)\n",
    "    mean, median, std = sigma_clipped_stats(image[0].data, sigma=3.0, mask=mask)\n",
    "    print((mean, median, std))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da24c6",
   "metadata": {},
   "source": [
    "I guess just look for outliers....\n",
    "\n",
    "cyx, (first try) some of the images are super duper noisy and quite a bit brighter than the others! (second try) Same! Plus tons of hot pixels now?\n",
    "\n",
    "koy: (R) Yeah, second one is noise but not crazy so. (U) noise is quite small, even though the signal looks pretty bad.\n",
    "\n",
    "sse: Some more, some less, seems fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef879c56",
   "metadata": {},
   "source": [
    "2D background estimator (50x50 box, 3x3 median filter). First image first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67804447",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bkg_estimator = MedianBackground()\n",
    "sigma_clip = SigmaClip(sigma=3.0)\n",
    "bkg = Background2D(image[0].data, (bg_size,bg_size), filter_size=(filter_size, filter_size),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)\n",
    "plt.imshow(bkg.background, origin='lower', cmap='Greys_r',interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76f946a",
   "metadata": {},
   "source": [
    "But this is unmasked, and it does look like the stars are impacting the background. So let's try the same with a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18c6e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bkg = Background2D(image[0].data, (bg_size,bg_size), filter_size=(filter_size, filter_size),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator,mask=mask)\n",
    "plt.imshow(bkg.background, origin='lower', cmap='Greys_r',interpolation='nearest')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9344289",
   "metadata": {},
   "source": [
    "yyyeaahh I guess that's better, the background should probably be NOT bumpy at all. So use this and background subtract everything.\n",
    "\n",
    "(Background goes negative here, I had real problems in AIJ with that in the past, so make a note of that!)\n",
    "\n",
    "sse: (large image) Uhhhh background was totally flat, which seems wrong...it's not just the scale, the mask must actually be bonkers. Start by trying unmasked background. (small image) looks way more even, let's do it. still failing, so let's try unmasked? No, unmasked went negative!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c275c",
   "metadata": {},
   "source": [
    "So actually, sometimes the unmasked background here looks *better*, at least with this first image. So I'm going to set a flag to decide which background to use. This should probably be set to default to the masked, however!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb40c41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do this for each image in the array.\n",
    "bkg_subtract_array=[]\n",
    "for i in range(len(align_image_array)):\n",
    "    image = fits.open(align_image_array[i])\n",
    "    hdr = image[0].header #save fits header\n",
    "    \n",
    "    # Basic background set up.\n",
    "    bkg_estimator = MedianBackground()\n",
    "    sigma_clip = SigmaClip(sigma=3.0)\n",
    "    threshold = detect_threshold(image[0].data, nsigma=2.0, sigma_clip=sigma_clip)\n",
    "    \n",
    "    # get source mask individually for each image (didn't have to do this on ctn!)\n",
    "    if bg_mask_flag==1:\n",
    "        # get source mask individually for each image (didn't have to do this on ctn!)\n",
    "        print(\"Subtracting background with a mask...\")\n",
    "        segment_img = detect_sources(image[0].data, threshold, npixels=10)\n",
    "        mask = segment_img.make_source_mask(footprint=footprint) # mask too big once?\n",
    "        # sometimes need to evaluate the mask...\n",
    "        #print(mask.sum())\n",
    "        # measure and subtract the background\n",
    "        # note this exclude_percentile is default 10, and more means more exclusions. From the Docs, \"For best results, \n",
    "        # exclude_percentile should be kept as low as possible \", although increasing has fixed a few errors.\n",
    "        bkg = Background2D(image[0].data, (bg_size,bg_size), filter_size=(filter_size,filter_size),\\\n",
    "                       sigma_clip=sigma_clip, bkg_estimator=bkg_estimator,mask=mask,exclude_percentile=10.0)\n",
    "    if bg_mask_flag==0:\n",
    "        # use unmasked background.\n",
    "        print(\"Subtracting background without a mask...\")\n",
    "        bkg = Background2D(image[0].data, (bg_size,bg_size), filter_size=(filter_size, filter_size),sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)\n",
    "\n",
    "    # write background file\n",
    "    bkg_subtract_name=align_image_array[i].replace(\".resamp.fits\", \"_bkgsub.fits\")\n",
    "    bkg_subtract_array.append(bkg_subtract_name)\n",
    "    if bg_subtract_pre_resamp_flag==0: # do this background subtraction...\n",
    "        image[0].data = image[0].data - bkg.background\n",
    "    elif bg_subtract_pre_resamp_flag==1: # don't do it, it's already been done!\n",
    "        print(\"Background subtraction already happened during resampling step!\\n\")\n",
    "    hdu_image_sub = fits.PrimaryHDU(image[0].data,image[0].header) #added headers! Includes SAT level!\n",
    "    hdu_image_sub.writeto(bkg_subtract_name,overwrite=True)\n",
    "    \n",
    "    print(\"Mean, Median, and Standard Deviation for the background\\n\\n\")\n",
    "    mean, median, std = sigma_clipped_stats(image[0].data, sigma=3.0, mask=mask)\n",
    "    print((mean, median, std))  \n",
    "    print(\"Writing background subtracted file\",bkg_subtract_name,\"\\n===============================================\")\n",
    "    #plotting for evaluation\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(32,32)) # could add figsize command...\n",
    "    plt.title(bkg_subtract_name)\n",
    "    mean, median, std = sigma_clipped_stats(bkg.background)\n",
    "    pcm=axs[0].imshow(bkg.background, vmin=median-2*std, vmax=median+2*std)\n",
    "    plt.colorbar(pcm,ax=axs[0],shrink=0.4)\n",
    "    mean, median, std = sigma_clipped_stats(image[0].data)\n",
    "    pcm=axs[1].imshow(image[0].data, vmin=median-2*std, vmax=median+2*std)\n",
    "    plt.colorbar(pcm,ax=axs[1],shrink=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed41df12",
   "metadata": {},
   "source": [
    "For cyx, (first time) unmasked background was *way* better, much less gradient present after subtraction. (1.6) Also true, unmasked looks better.\n",
    "\n",
    "koy: (R) Yeah second image has some kind of crazy arc in it. Leaving it for now. (U) Ok, these look *ok*, but not amazing. They look aligned, at least, but the target is totally invisible.\n",
    "\n",
    "\n",
    "sse: Mostly seems ok, slightly noiser around edges. Progressing for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a4e78",
   "metadata": {},
   "source": [
    "These images look right in AIJ, but I did get an error that these images are now double precision, whereas AIJ expects single precision. Fine for now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3d2ced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bkg_subtract_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd53420",
   "metadata": {},
   "source": [
    "## PSF matching\n",
    "\n",
    "The atmosphere heavily affects the PSF of the images by determining the \"seeing\" conditions. The seeing for ground-based optical telescopes is usually measured as the FWHM of the imaging PSF.  Properties of the atmosphere can change very rapidly, so it is rare that science and reference images are characterized by the same seeing. Therefore their PSFs are usually different, which is a problem for image subtraction. \n",
    "\n",
    "\n",
    "### Generate the kernel for the convolution\n",
    "\n",
    "The PSF of the science and reference images can be matched in several different ways.  Here we start by performing a first source extraction on the reference image (which we assume to be the last in the list).  We can use the catalogs of sources that we obtain for two main purposes: <br />\n",
    "1. Measure the PSF of the science reference frame, using PSFex or photutils\n",
    "2. Obtain instruments magnitudes that will be the basis for the zero-point calibration (see Photometry module).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block for masking out the saturated stars.\n",
    "\n",
    "if sat_mask_flag==1:\n",
    "    for i in range(len(bkg_subtract_array)):\n",
    "        # grab the saturation level ...\n",
    "        file_name=os.path.join(proc_dir, bkg_subtract_array[i])\n",
    "        # get saturation level\n",
    "        image = fits.open(file_name) # only need open to read the header\n",
    "        SAT=image[0].header['SATURATE']\n",
    "        print(\"Saturation level\",SAT)\n",
    "        # Check histogram?\n",
    "        #mean, median, std = sigma_clipped_stats(image[0].data)\n",
    "        #plt.imshow(image[0].data,vmin=median-2*std,vmax=median+2*std)\n",
    "        #plt.show()\n",
    "        #hist_data=(image[0].data).flatten()\n",
    "        #print(hist_data.shape,(image[0].data).shape)\n",
    "        #plt.hist(hist_data, bins=1000,range=(0,40000))\n",
    "        #plt.show()\n",
    "\n",
    "        \n",
    "        # No, this block needs to mask out a CIRCLE around the saturated source, otherwise we get those missing\n",
    "        # black spots in the PSF...\n",
    "\n",
    "        # set the threshhold to like 80% of that level....\n",
    "        limit=SAT_FRAC*SAT\n",
    "        # use segment images to mask the sources which have any pixels over that 80% level...\n",
    "        # 1 connected pixels greater than the limit. Not that a larger value here means LESS masking.\n",
    "        # npixels=4 worked before, but found nothing in B filter...\n",
    "        segment_map = detect_sources(image[0].data, limit, npixels=pixmask)\n",
    "        #print(segment_map)\n",
    "        #plt.imshow(segment_map, cmap=segment_map.cmap,interpolation='nearest')\n",
    "        #plt.show()\n",
    "        # ...\n",
    "        # profit?\n",
    "        if (segment_map == None): # if there are no saturated sources, skip this!\n",
    "            masked_image=image[0].data\n",
    "        else:\n",
    "            mask_bool=segment_map.make_source_mask()\n",
    "            masked_image=np.where(mask_bool==False,image[0].data,np.NaN)\n",
    "        #mean, median, std = sigma_clipped_stats(masked_image)\n",
    "        #plt.imshow(masked_image,vmin=median-2*std,vmax=median+2*std)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2,figsize=(16,16)) # could add figsize command...\n",
    "        plt.title(file_name)\n",
    "        mean, median, std = sigma_clipped_stats(mask_bool)\n",
    "        pcm=axs[0].imshow(mask_bool, vmin=median-2*std, vmax=median+2*std)\n",
    "        plt.colorbar(pcm,ax=axs[0],shrink=0.4)\n",
    "        mean, median, std = sigma_clipped_stats(masked_image)\n",
    "        pcm=axs[1].imshow(masked_image, vmin=median-2*std, vmax=median+2*std)\n",
    "        plt.colorbar(pcm,ax=axs[1],shrink=0.4)\n",
    "\n",
    "        # now write the masked image to the original image location, so the names don't have to get messed around with.\n",
    "        hdu_image_sub = fits.PrimaryHDU(masked_image,image[0].header) #added headers! Includes SAT level!\n",
    "        hdu_image_sub.writeto(file_name,overwrite=True) # overwrite the files so we can continue without change.\n",
    "\n",
    "else:\n",
    "    print(\"Vanilla routine, saturated stars NOT masked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13309d98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use REFERENCE image to extract sources.\n",
    "# overwriting the catalog, but I guess that doesn't matter much.\n",
    "\n",
    "#ref_image_bkgsub_name = ref_bkg_subtract_name # SWTICH when you go back to multi image!\n",
    "ref_image_bkgsub_name = os.path.join(proc_dir, bkg_subtract_array[len(bkg_subtract_array)-1])\n",
    "\n",
    "if os.path.exists('prepsfex.cat'): #Remove possible temporary files\n",
    "    os.remove(\"prepsfex.cat\") \n",
    "\n",
    "# get saturation level\n",
    "ref_image_bkgsub = fits.open(ref_image_bkgsub_name) # only need open to read the header\n",
    "SAT=ref_image_bkgsub[0].header['SATURATE']\n",
    "print(\"Saturation level\",SAT)\n",
    "    \n",
    "try:\n",
    "    #command = \"sextractor %s -c %s -CATALOG_NAME %s -MAG_ZEROPOINT 25.0\" % (sci_image_aligned_name, os.path.join(data_dir, 'prepsfex.sex'), os.path.join(proc_dir, 'prepsfex.cat'))\n",
    "    #command = \"source-extractor %s -c %s -CATALOG_NAME %s -MAG_ZEROPOINT 25.0\" % (sci_image_aligned_name, os.path.join(data_dir, 'prepsfex.sex'), os.path.join(proc_dir, 'prepsfex.cat'))\n",
    "    command = \"source-extractor %s -c %s -CATALOG_NAME %s -MAG_ZEROPOINT 25.0 -SATUR_LEVEL %s\" % (ref_image_bkgsub_name, \\\n",
    "        os.path.join(data_dir,'prepsfex.sex'), os.path.join(proc_dir, 'prepsfex.cat'),SAT)\n",
    "    print('Executing command: %s\\n' % command)\n",
    "    rval = subprocess.run(command.split(), check=True)\n",
    "    print('Success!')\n",
    "except subprocess.CalledProcessError as err:\n",
    "    print('Could not run SExtractor with exit error %s'%err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20078df2",
   "metadata": {},
   "source": [
    "Saturation filter pre-PSF, didn't do much so generally not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for saturation (ANY 4, even in another number, indicates a saturated pixel):\n",
    "# But note from the docs:  https://sextractor.readthedocs.io/en/latest/Flagging.html these flags are\n",
    "# \"in order of increasing concern\", so maybe the right is to filter out 4 AND HIGHER?\n",
    "# Need the lines above for this!\n",
    "'''\n",
    "filter_sat=1 # 0 to NOT filter out saturation flags, 1 to YES filter for saturation flags.\n",
    "j=0\n",
    "for i in reversed(range(len(cat_data['FLAGS']))): #loop backwards so we can remove rows\n",
    "    if cat_data['FLAGS'][i]==4:\n",
    "        if filter_sat==1:\n",
    "            cat_data.remove_row(i)\n",
    "        j=j+1\n",
    "    \n",
    "j\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab9ab7",
   "metadata": {},
   "source": [
    "koy: (R) 52 sources from 727 have the saturation flag, but note this just removes the 4 flags ONLY - if they have multiple flags they are untouched. The saturation flag filtering did NOT help the alignment issue later. (U) no saturated flags?\n",
    "\n",
    "sse: 153 of 1533 saturated flags on default settings.\n",
    "\n",
    "From \"Sextractor for dummies\": https://arxiv.org/pdf/astro-ph/0512139\n",
    "\n",
    "\"SATUR LEVEL is the limit for SE to start extrapolating to get the photometry. However as\n",
    "soon as you hit something as saturated as that you might want to see if there is another way to\n",
    "determine the flux from that object.\"\n",
    "\n",
    "So yes - it's just flagged. Good for catalog, but not good for any calculations!\n",
    "\n",
    "PSFex: It seems to be ignoring ANY flagged sources, at least by default. I am going to change the parameter file, \n",
    "\n",
    "SAMPLE_MINSN       3            # Minimum S/N for a source to be used\n",
    "\n",
    "To 20 (program default), and check the effect! No improvement! 50? Nothing! That's not the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31db58",
   "metadata": {},
   "source": [
    "I guess take a look at the catalog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f150550",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "#ax = fig.add_subplot(111)\n",
    "plt.scatter(cat_data['X_IMAGE'], cat_data['Y_IMAGE'], facecolors='none', edgecolors='r', linewidths=0.5)\n",
    "#image = fits.open(align_image_array[i])\n",
    "mean, median, std = sigma_clipped_stats(ref_image_bkgsub[0].data)\n",
    "plt.imshow(ref_image_bkgsub[0].data, vmin=median-2*std, vmax=median+2*std)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d366c07",
   "metadata": {},
   "source": [
    "I think I would like a way to remove the saturated sources, since those might be causing the alignment problems I'm having with koy. \n",
    "\n",
    "Now I have removed them, but clearly not all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47cb8d",
   "metadata": {},
   "source": [
    "Now we use another software part of the AstrOmatic suite, PSFex, to **measure the PSF of the reference image**. PSFex estimates the PSF based on the information present in the catalog generated with SExtractor.  Then, let's plot the PSF model obtained with PSFex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662cc261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run PSFex to compute PSF, read and display the final model; needs to output to \"out\" dir.\n",
    "if not os.path.isdir('out'): os.mkdir('out')\n",
    "\n",
    "try:\n",
    "    command = \"psfex prepsfex.cat -c psfex_conf.psfex -PSF_SIZE=%s, %s -VERBOSE_TYPE=FULL\" % (size_of_psf,size_of_psf)\n",
    "    \n",
    "    #command = \"psfex %s -c %s\" % (source_extract_cat_name,psfex_config_name)\n",
    "    print('Executing command: %s\\n' % command)\n",
    "    rval = subprocess.run(command.split(), check=True)\n",
    "    print('Success!')\n",
    "except subprocess.CalledProcessError as err:\n",
    "    print('Could not run psfex with exit error %s'%err)\n",
    "\n",
    "psf_ref_image_name = os.path.join(out_dir,'proto_prepsfex.fits') # hard coded name from psfex?\n",
    "print(psf_ref_image_name)\n",
    "psf_ref_image = fits.open(psf_ref_image_name)\n",
    "\n",
    "mean, median, std = sigma_clipped_stats(psf_ref_image[0].data[0])\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(psf_ref_image[0].data[0], vmin=median-2*std, vmax=median+2*std)\n",
    "plt.colorbar(shrink = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43cad4",
   "metadata": {},
   "source": [
    "cyx: (1.?) Another pretty bad PSF! Ok, but the median filtering helped! (1.6) hole in the middle? Fixed with a median on the reference, but still looks quite bad. Jesus finally, just screwing up WCS, B instead of I, etc.\n",
    "\n",
    "koy: Seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a1102",
   "metadata": {},
   "source": [
    "### Convolve the science images with the PSF of the reference image\n",
    "\n",
    "Now that the kernel is generated, let's convolve the science images with the PSF of the reference, to create our array of convolved science images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c5a279",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convolve the kernel of the reference image PSF with each science frame\n",
    "kernel_ref = psf_ref_image[0].data[0]\n",
    "\n",
    "# for plotting\n",
    "#fig, axes = plt.subplots(3, math.ceil(len(bkg_subtract_array)/3.))\n",
    "#plt.figure(figsize=(4,4))\n",
    "\n",
    "sci_conv_array=[]\n",
    "for i in range(len(bkg_subtract_array)-1): # don't need to do the last one, that's reference\n",
    "    sci_bkg_subtract = fits.open(bkg_subtract_array[i])\n",
    "    if sat_mask_flag==1: # can't do \"fft\" method, I guess! \n",
    "        sci_conv=convolve(sci_bkg_subtract[0].data, kernel_ref)\n",
    "        # scipy convolve copies all the NaNs into the convolution.\n",
    "        #sci_conv = scipy_convolve(sci_bkg_subtract[0].data, kernel_ref, mode='same', method='direct')\n",
    "    else:\n",
    "        sci_conv = scipy_convolve(sci_bkg_subtract[0].data, kernel_ref, mode='same', method='fft') # convolve step\n",
    "    hdu_sci_conv = fits.PrimaryHDU(sci_conv,sci_bkg_subtract[0].header) # really should be adding header keys!\n",
    "    sci_conv_array.append(bkg_subtract_array[i].replace(\".fits\",\"_conv.fits\"))\n",
    "    sci_conv_name=os.path.join(proc_dir,sci_conv_array[i])\n",
    "    \n",
    "    # Write the images\n",
    "    print(sci_conv_name)\n",
    "    hdu_sci_conv.writeto(sci_conv_name,overwrite=True)\n",
    "\n",
    "    #Plot up the convolved reference image\n",
    "    mean, median, std = sigma_clipped_stats(hdu_sci_conv.data)\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # set the scale of the image based on its statistics\n",
    "    # Name looks totally wrong!\n",
    "    plt.imshow(hdu_sci_conv.data, vmin=median-2*std, vmax=median+2*std)\n",
    "    plt.colorbar(shrink = 0.4)\n",
    "    plt.title('Convolved science image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82c4d7",
   "metadata": {},
   "source": [
    "This step was borked for a while, and displaced the PRE-convolution images. These always look bad, on what looks like the saturated stars. I think this makes sense - when you have saturation, the PSFs are \"too wide\", and the presumably the psf generator is getting rid of some saturated sources.\n",
    "\n",
    "koy: (U) These are clearly \"right\", but the noise is pretty high and this might be why the resulting images don't look so good.\n",
    "\n",
    "sse: Looks pretty good.\n",
    "\n",
    "Next step is actually to create a corresponding refernce image for each science image, which is the reference convolved with the science. Then each pair should have the same PSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee66bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sci_conv_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1511a",
   "metadata": {},
   "source": [
    "### Convolve the reference image with the PSF of each science image\n",
    "\n",
    "\n",
    "Same as above, but this time we generate a kernel with the properties of the **PSF of the science images**.  Then, we convolve the reference image with this kernel, once for each science image. This creates the array of convolved reference images, which are needed to subtract from the science frames.\n",
    "\n",
    "I think what is happening here is the PSF data is stored in prepsfex.cat, as an output catalog from source-extractor, so we need to re-run the source extractor to get the PSF of the image in question. \n",
    "\n",
    "Kernel from *each* science image\n",
    "\n",
    "Maybe what needs to go here is a way to CLEAN the catalogs, removing sources that are clearly saturated, so they are not used in the normalization routine?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc5e51",
   "metadata": {},
   "source": [
    "During this step, I once got an error (on sse) not finding sources with appropriate FWHM...I fixed that by adding -DETECT_MINAREA 1 to the source-extractor command (default 5), or -DETECT_THRESH 3...the second one looked a bit better. Basically, the seeing was just *too good* for that night! That's this line in the 3rd box wwwaaayyy above:\n",
    "\n",
    "extract_thresh=5 # Threshhold for detection above sigma - default 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6d6ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ref_conv_array=[]\n",
    "for i in range(len(bkg_subtract_array)-1): # don't need to do the last one, that's the reference\n",
    "    sci_image_bkgsub_name = os.path.join(proc_dir, bkg_subtract_array[i])\n",
    "    sci_image_bkgsub = fits.open(sci_image_bkgsub_name) # only need open to read the header\n",
    "    print(\"=======================================================\\n\\nThis Image info:\") # for debugging\n",
    "    sci_image_bkgsub.info() # for debugging\n",
    "\n",
    "    try:\n",
    "        # prepsfex.sex is the config file\n",
    "        # prepsfex.cat is a binary output file, gets rewriten each time.\n",
    "        # let's try to put the saturation level into this command.\n",
    "        # didn't fix anything!\n",
    "        # BACK_SIZE also doesn't seem to help this problem.\n",
    "        SAT=sci_image_bkgsub[0].header['SATURATE']\n",
    "        print(\"Saturation level\",SAT)\n",
    "        command = \"source-extractor %s -c %s -CATALOG_NAME %s -MAG_ZEROPOINT 25.0 -SATUR_LEVEL %s -DETECT_THRESH %s\" % (sci_image_bkgsub_name, \\\n",
    "            os.path.join(data_dir,'prepsfex.sex'), os.path.join(proc_dir, 'prepsfex.cat'),SAT,extract_thresh)\n",
    "        \n",
    "        print('Executing command: %s\\n' % command)\n",
    "        rval = subprocess.run(command.split(), check=True)\n",
    "        print('Success!')\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print('Could not run SExtractor with exit error %s'%err)\n",
    "     \n",
    "    # Run PSFex to compute PSF, read and display the final model; needs to output to \"out\" dir.\n",
    "    if not os.path.isdir('out'): os.mkdir('out') # should be made already\n",
    "\n",
    "    try:\n",
    "        # prepsfex.cat is the binary file created in the last step.\n",
    "        # psfex_conf.psfex is config file, used each time.\n",
    "        \n",
    "        # can I filter out the saturated sources here, so they are not used for finding the PSF?\n",
    "        \n",
    "        command = \"psfex prepsfex.cat -c psfex_conf.psfex -PSF_SIZE=%s, %s -VERBOSE_TYPE=FULL\" % (size_of_psf,size_of_psf)\n",
    "        print('Executing command: %s\\n' % command)\n",
    "        rval = subprocess.run(command.split(), check=True)\n",
    "        print('Success!')\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print('Could not run psfex with exit error %s'%err)\n",
    "\n",
    "    psf_sci_image_name = os.path.join(out_dir,'proto_prepsfex.fits') # hard coded name\n",
    "    #psf_sci_image_name = os.path.join(out_dir,bkg_subtract_array[i].replace('.fits','_protoPSF.fits')) # rewriting each time!\n",
    "    #print(psf_sci_image_name)\n",
    "    psf_sci_image = fits.open(psf_sci_image_name)\n",
    "    # only the last image shows...\n",
    "    mean, median, std = sigma_clipped_stats(psf_sci_image[0].data[0])\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(psf_sci_image[0].data[0],   vmin=median-2*std, vmax=median+2*std)  \n",
    "    plt.colorbar(shrink = 0.4)\n",
    "    plt.title('Science Image PSF')\n",
    "    plt.show()   \n",
    "        \n",
    "    # Convolve the reference image PSF with the Pof each science frame\n",
    "    # Grab the kernel!\n",
    "    kernel_sci = psf_sci_image[0].data[0]\n",
    "    \n",
    "    # careful naming scheme needed!\n",
    "    ref_bkg_subtract = fits.open(bkg_subtract_array[len(bkg_subtract_array)-1]) # REFERENCE IS LAST IMAGE!\n",
    "    print(\"Check: Reference image here is\",bkg_subtract_array[len(bkg_subtract_array)-1])\n",
    "    if sat_mask_flag==1: # can't do \"fft\" method, I guess!\n",
    "        ref_conv=convolve(sci_bkg_subtract[0].data, kernel_sci)\n",
    "        # scipy convolve copies all the NaNs into the convolution.\n",
    "        #ref_conv = scipy_convolve(ref_bkg_subtract[0].data, kernel_sci, mode='same', method='direct') # convolve step\n",
    "    else:\n",
    "        ref_conv = scipy_convolve(ref_bkg_subtract[0].data, kernel_sci, mode='same', method='fft') # convolve step\n",
    "    \n",
    "    hdu_ref_conv = fits.PrimaryHDU(ref_conv,ref_bkg_subtract[0].header) # header keys don't super matter...but from the reference!\n",
    "    \n",
    "    # yikes, this has to be named CAREFULLY!\n",
    "    ref_conv_array.append(bkg_subtract_array[i].replace(\".fits\",\"_refconv.fits\"))\n",
    "    ref_conv_name=os.path.join(proc_dir,ref_conv_array[i])\n",
    "    \n",
    "    # Create the images\n",
    "    print(ref_conv_name)\n",
    "    hdu_ref_conv.writeto(ref_conv_name,overwrite=True)\n",
    "\n",
    "    #Plot up the convolved reference image\n",
    "    mean, median, std = sigma_clipped_stats(hdu_ref_conv.data)\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    # include the catalog\n",
    "    cat = fits.open(\"prepsfex.cat\")\n",
    "    cat_data = Table(cat[2].data)\n",
    "    plt.scatter(cat_data['X_IMAGE'], cat_data['Y_IMAGE'], facecolors='none', edgecolors='r', linewidths=0.5)\n",
    "    \n",
    "    # set the scale of the image based on its statistics\n",
    "    plt.imshow(ref_conv, vmin=median-2*std, vmax=median+2*std)\n",
    "    plt.colorbar(shrink = 0.4)\n",
    "    plt.title('Convolved reference image for this science frame:')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88945e9a",
   "metadata": {},
   "source": [
    "Checking in AIJ, the resulting images have nearly the same PSF! (checking a few example sources, and they even seem to have the same pixel location values, which is good for us!\n",
    "\n",
    "bright stars do have halos, which might be screwing up the background calculation, or the PSF? If we are eventually going to just subtract a circle, this might not matter.\n",
    "\n",
    "\n",
    "sse: (large image) the first image has a bonkers PSF, with a giant line through it! (smaller image) fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8ecdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ref_conv_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4bcd7",
   "metadata": {},
   "source": [
    "### Improving the alignment\n",
    "Now that the science image is convolved with (an approximation of) the PSF of the reference image, \n",
    "and the reference image is convolved with the PSF of the science image, we can perform the image subtraction.\n",
    "\n",
    "- Before the subtraction we use an fft method (chi_2_shift) to fine-tune the image alignment of the reference and science image. We'll do this in pairs, reference aligning with science. When we go do photometry with AIJ, this fine tuning doesn't matter so much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f904a",
   "metadata": {},
   "source": [
    "## Normalization and Subtraction of the images\n",
    "\n",
    "The science and reference images are usually obtained with different exposure times.  In addition, the reference image can be the stack of several images to increase the depth.  Finally, different CCDs of the same camera (or even different regions of the same CCD when multiple amplifiers are present) may have slightly different gain. <br >\n",
    "\n",
    "The background subtraction should have removed the non-linear offsets between science and reference images.  We can therefore normalize the two images by computing the ratio of bright star fluxes in the two images. Once again, we use SExtractor to extract the flux and other quantities.\n",
    "\n",
    "We should be able to do this pairwise as well, as the offsets between different science images should be taken care of with our differential photometry. So we'll maybe normalize the reference(s) to the science?\n",
    "\n",
    "Adding a catalog filter here, to get rid of things with flags over 4.\n",
    "\n",
    "Have several other types of filters (and debugging info) down there, but no evidence that any of them actually helps anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning - do this before matching, I think.\n",
    "# Catalog expected is catalog filename from sextractor.\n",
    "# Nah, I think this has to be done as ASCII to match the rest of the code!\n",
    "# Maybe read it in twice - once to get the header (as \"basic\"), and once to get the data (as \"sextractor\").\n",
    "# For the flags we need the sextractor format, while for the data we need ascii.\n",
    "# so we will probably have to write as ascii....or correlate the rows between the two?\n",
    "def clean_cat(catalog):\n",
    "    cat_sci_sex = ascii.read(catalog,format='sextractor')\n",
    "    cat_sci_basic = ascii.read(catalog,format='no_header') \n",
    "    print(\"Initial catalog length: \",len(cat_sci_sex),\",\",len(cat_sci_basic),\" (for debug, should be the same!)\")\n",
    "    j=0\n",
    "    for i in reversed(range(len(cat_sci_sex))): #loop backwards in sextractor catalog so we can remove rows\n",
    "        if cat_sci_sex['FLAGS'][i]>=4:\n",
    "            cat_sci_basic.remove_row(i) # remove the row in the basic catalog\n",
    "        if cat_sci_sex['FLUX_AUTO'][i]<=0: # remove if flux drops below zero\n",
    "            cat_sci_basic.remove_row(i) # remove the row in the basic catalog\n",
    "        j=j+1\n",
    "        #print(j)\n",
    "    print(\"Filtered down to length:\", len(cat_sci_basic))\n",
    "    cat_sci_basic.write(catalog,overwrite=True,format='ascii.no_header') # hope the headers get written?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967999f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We wrote a routine to filter the catalogs before normalization to remove flagged sources (generally saturated or\n",
    "# other bad things) but that did not seem to improve the situation here (sse), so we are giving up! Change this flag\n",
    "#if you like, but no evidence that it actually helps!\n",
    "#filter_cat_flag=1\n",
    "# in theoutlier elimination, also - reduced the spread over the normalization, but didn't actually improve\n",
    "# the resulting images! arg! This gives a 3 sigma filter on the flux ratios.\n",
    "#eliminate_outliers_flag=1\n",
    "#show_images_flag=1\n",
    "sub_array=[]\n",
    "#for i in range(len(bkg_subtract_array)-1): \n",
    "for i in range(len(sci_conv_array)): # changed this line!\n",
    "    \n",
    "    # on dpj some of the galaxy core was left after subtraction, so going to add the saturation\n",
    "    # keyword to the source extractor, so maybe it will skip some bright sources and the normalization\n",
    "    # will be better....nope, this did not work!\n",
    "    sci_image = fits.open(sci_conv_array[i])\n",
    "    SAT=sci_image[0].header['SATURATE']\n",
    "    print(\"Saturation level on Science:\",SAT)\n",
    "    # Run SExtractor on the science image\n",
    "    if os.path.exists('sci_match.cat'): #Remove possible temporary files - don't think I actually need this!\n",
    "        os.remove(\"sci_match.cat\") \n",
    "    # watch the order on these parameters, apparently putting saturation at the end dooesn't work!\n",
    "    # NOte these are explicitly being done as ASCII!\n",
    "    sextractor_command = \"source-extractor %s -c prepsfex.sex -CATALOG_NAME sci_match.cat -MAG_ZEROPOINT 25.0 -SATUR_LEVEL %s -DETECT_THRESH %s -CATALOG_TYPE=ASCII_HEAD\" % (sci_conv_array[i],SAT,extract_thresh)\n",
    "    #sextractor_command = \"source-extractor %s -c prepsfex.sex -CATALOG_NAME sci_match.cat -MAG_ZEROPOINT 25.0 -CATALOG_TYPE=ASCII_HEAD\" % (sci_conv_array[i])\n",
    "    \n",
    "    try:\n",
    "        command = sextractor_command\n",
    "        print('Executing command: %s\\n' % command)\n",
    "        #command = \"source-extractor %s -c %s -CATALOG_NAME %s -MAG_ZEROPOINT 25.0 -SATUR_LEVEL %s\" % (sci_conv_array[i], \\\n",
    "        #os.path.join(data_dir,'prepsfex.sex'), os.path.join(proc_dir, 'sci_match.cat'),SAT)\n",
    "        rval = subprocess.run(command.split(), check=True)\n",
    "        print('Success!')\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print('Could not run SExtractor with exit error %s'%err)\n",
    "\n",
    "    if (filter_cat_flag==1): clean_cat(\"sci_match.cat\")\n",
    "    cat_sci = ascii.read('sci_match.cat') # dummy, should be able to rewrite constantly\n",
    "    \n",
    "    ref_image = fits.open(ref_conv_array[i])\n",
    "    SAT=ref_image[0].header['SATURATE']\n",
    "    print(\"Saturation level on Reference\",SAT)\n",
    "    # Run SExtractor on the reference image\n",
    "    sextractor_command = \"source-extractor %s -c prepsfex.sex -CATALOG_NAME ref_match.cat -MAG_ZEROPOINT 25.0 -SATUR_LEVEL %s -DETECT_THRESH %s -CATALOG_TYPE=ASCII_HEAD\" % (ref_conv_array[i],SAT,extract_thresh)\n",
    "    #sextractor_command = \"source-extractor %s -c prepsfex.sex -CATALOG_NAME ref_match.cat -MAG_ZEROPOINT 25.0 -CATALOG_TYPE=ASCII_HEAD\" % (ref_conv_array[i])\n",
    "    \n",
    "    try:\n",
    "        command = sextractor_command\n",
    "        print('Executing command: %s\\n' % command)\n",
    "        rval = subprocess.run(command.split(), check=True)\n",
    "        print('Success!')\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print('Could not run SExtractor with exit error %s'%err)\n",
    "\n",
    "    # Read in the SExtractor output catalog\n",
    "    if (filter_cat_flag==1): clean_cat(\"ref_match.cat\")\n",
    "    cat_ref = ascii.read('ref_match.cat')\n",
    "    \n",
    "    # Match the catalog of sources of the reference and science images.  \n",
    "    # Calculate the ratio between the flux of source in the science image over the flux of sources \n",
    "    # in the reference image.\n",
    "    \n",
    "    c_sci = SkyCoord(ra=cat_sci['X_WORLD'], dec=cat_sci['Y_WORLD'])\n",
    "    c_ref = SkyCoord(ra=cat_ref['X_WORLD'], dec=cat_ref['Y_WORLD'])\n",
    "\n",
    "    # I think idx is the index of c_ref that each c_sci matches into.\n",
    "    idx, d2d, d3d = c_sci.match_to_catalog_3d(c_ref)\n",
    "\n",
    "    # Initialize a list for the indexes and one for the flux ratios\n",
    "    # for each (sci, ref) pair, don't need to save all this?\n",
    "    index_arr = []\n",
    "    ratio_arr = []\n",
    "\n",
    "    # for debugging\n",
    "    sci_list=[]\n",
    "    ref_list=[]\n",
    "    \n",
    "    # Sure don't need to be printing all this stuff! Is there a final normalization i can use for checking?\n",
    "    for j, i2, d in zip(idx, np.arange(len(d2d)),d2d):\n",
    "        #print(i,d)\n",
    "        #index_arr.append(i)\n",
    "        \n",
    "        # for debugging #\n",
    "        #print(\"Image coordinates | separation\")\n",
    "        #print(cat_ref['X_IMAGE'][j],cat_ref['Y_IMAGE'][j],'  ', cat_sci['X_IMAGE'][i2],cat_sci['Y_IMAGE'][i2],d)\n",
    "        #print('Fluxes and flux ratio')\n",
    "        #print(cat_ref['FLUX_AUTO'][j], cat_sci['FLUX_AUTO'][i2],'                  ', cat_sci['FLUX_AUTO'][i2] / cat_ref['FLUX_AUTO'][i])\n",
    "        ##\n",
    "        \n",
    "        # For debugging\n",
    "        sci_list.append(cat_sci['FLUX_AUTO'][i2])\n",
    "        ref_list.append(cat_ref['FLUX_AUTO'][j])\n",
    "        \n",
    "        ratio_arr.append(cat_sci['FLUX_AUTO'][i2] / cat_ref['FLUX_AUTO'][j]) # Ratio of SCI/REF!\n",
    "    \n",
    "    if (show_images_flag==1):\n",
    "        plt.xlabel('Source Number')\n",
    "        plt.ylabel('Flux')\n",
    "        plt.plot(sci_list)\n",
    "        plt.plot(ref_list)\n",
    "        plt.show()\n",
    "    \n",
    "    #\n",
    "    # Current thought is the issue is here - the median vs mean of the scaling factors are vastly\n",
    "    # different (factor of 4) as opposed to the case for dpj, when they are like 0.9-1.1. \n",
    "    # Maybe going back and carrying around a correct value of the saturate keyword will help?\n",
    "    # another alternative is to go back to the background of the reference image - it's just worse!\n",
    "    #\n",
    "    \n",
    "    \n",
    "    if (eliminate_outliers_flag==1):\n",
    "        fig, axs = plt.subplots(1, 2,figsize=(8,8))\n",
    "        im0=axs[0].set_title('Pre-cleaned flux ratios')\n",
    "        im0=axs[0].plot(ratio_arr)\n",
    "        remove = np.where(ratio_arr>np.average(ratio_arr)+3*np.std(ratio_arr))\n",
    "        print(\"removing...\",remove)\n",
    "        ratio_arr = np.delete(ratio_arr, remove)\n",
    "        #for i in reversed(range(len(ratio_arr))): # this should go backward...\n",
    "        #    print(ratio_arr[i],np.average(ratio_arr),np.std(ratio_arr))\n",
    "        #    if ( ratio_arr[i]>np.average(ratio_arr)+3*np.std(ratio_arr) ) or ( ratio_arr[i]<np.average(ratio_arr)-3*np.std(ratio_arr) ):\n",
    "        #        print(\"eliminating ratio:\", ratio_arr[i])\n",
    "        #        ratio_arr[i].remove()\n",
    "        im1=axs[1].set_title('Cleaned flux ratios')\n",
    "        im1=axs[1].plot(ratio_arr)\n",
    "        plt.show()\n",
    "    \n",
    "    scale = np.median(ratio_arr)\n",
    "    #scale=np.average(ratio_arr) # changing to average didn't seem to help\n",
    "    print(\"The scaling factor is\", scale,\"average:\",np.average(ratio_arr),\" deivation\",np.std(ratio_arr))\n",
    "    \n",
    "    # plotting for evaluation\n",
    "    #plt.hist(ratio_arr)\n",
    "    #plt.show()\n",
    "    \n",
    "    # fine tune the alignment here - at least without this, the alginment is terrible!\n",
    "    #sci_image = fits.open(sci_conv_array[i])\n",
    "    #ref_image = fits.open(ref_conv_array[i])\n",
    "    \n",
    "    # For testing alignment on koy\n",
    "    #xoff, yoff, exoff, eyoff = chi2_shift(ref_image[0].data, sci_image[0].data, 10, return_error=True, upsample_factor='auto')\n",
    "    # errors are just flat zeros here, so not likely they are being calculated carefully.\n",
    "    # no, those zeros are found in other sources as well...\n",
    "    xoff, yoff, exoff, eyoff = chi2_shift(ref_image[0].data, sci_image[0].data, 10, return_error=True, upsample_factor=10)\n",
    "    print(\"Alignment offsets:\",xoff,yoff,exoff,eyoff)\n",
    "    sci_image_shift = scipy.ndimage.shift(sci_image[0].data, [-yoff, -xoff], order=3, mode='reflect', cval=0.0, prefilter=True)\n",
    " \n",
    "    # Create an empty image with the reference image in a square in the center.\n",
    "    # \"synthetic image\"\n",
    "    #RA=128.249\n",
    "    #Dec=-27.449311\n",
    "    coord = SkyCoord(RA,Dec,unit='deg')\n",
    "    w = WCS(sci_image[0].header)\n",
    "    x, y = w.world_to_pixel(coord)\n",
    "    print(\"The source is at at: \",x,y)\n",
    "\n",
    "    # The synthetic image will be an image that starts with zeros everywhere, but with the windowed reference image\n",
    "    # at the center.\n",
    "    synthetic_image=np.zeros(sci_image[0].data.shape) \n",
    "    synthetic_image[int(y)-window:int(y)+window,int(x)-window:int(x)+window]=ref_image[0].data[int(y)-window:int(y)+window,int(x)-window:int(x)+window]\n",
    "    \n",
    "    # Do we need to plot the science and reference image here, as part of the debug?\n",
    "    if (show_images_flag==1):\n",
    "        fig, axs = plt.subplots(1, 2,figsize=(16,16))\n",
    "        im0=axs[0].set_title('Science Image')\n",
    "        im0=axs[0].imshow(sci_image_shift)\n",
    "        fig.colorbar(im0,ax=axs[0],shrink=0.25)\n",
    "        im1=axs[1].set_title('Reference (NOT Sythnetic Image)')\n",
    "        im1=axs[1].imshow(ref_image[0].data)\n",
    "        fig.colorbar(im1,ax=axs[1],shrink=0.25)\n",
    "        plt.show()\n",
    "    \n",
    "    # image subtraction step\n",
    "    # Rescale the reference image to the science and perform the image subtraction.\n",
    "    image_sub=sci_image_shift-synthetic_image*scale\n",
    "    \n",
    "    #plot step for evaluation\n",
    "    # JEsus christ, we have to get the titles in before the imshow command or the colorbar doens't know what to do!\n",
    "    fig, axs = plt.subplots(1, 3,figsize=(16,16))\n",
    "    im0=axs[0].set_title('Science Image Subtraction Region')\n",
    "    im0=axs[0].imshow(sci_image_shift[int(y)-window:int(y)+window,int(x)-window:int(x)+window])\n",
    "    fig.colorbar(im0,ax=axs[0],shrink=0.25)\n",
    "    #plt.show()\n",
    "    \n",
    "    im1=axs[1].set_title('(scaled) Synthetic Image Subtraction Region')\n",
    "    im1=axs[1].imshow(scale*synthetic_image[int(y)-window:int(y)+window,int(x)-window:int(x)+window])\n",
    "    fig.colorbar(im1,ax=axs[1],shrink=0.25)\n",
    "    #plt.show()\n",
    "    \n",
    "    im2=axs[2].set_title('Subtracted Image Subtraction Region')\n",
    "    #x,y=w.world_to_pixel(coord)\n",
    "    \n",
    "    im2=axs[2].imshow(image_sub[int(y)-window:int(y)+window,int(x)-window:int(x)+window])\n",
    "    im2=axs[2].scatter(window, window, marker='o', color='r', facecolors='none',s=1000,linewidths=2)\n",
    "    #axs[2].colorbar(shrink = 0.4)\n",
    "    fig.colorbar(im2,ax=axs[2],shrink=0.25)\n",
    "    plt.show()\n",
    "    #plt.tight_layout(h_pad=1)\n",
    "    \n",
    "    #image_sub=sci_image_shift-synthetic_image*scale\n",
    "    hdu_image_sub = fits.PrimaryHDU(image_sub,sci_image[0].header) # header might need work!\n",
    "    sub_array.append(sci_conv_array[i].replace(\".fits\",\"_diff.fits\")) \n",
    "    hdu_image_sub.writeto(\"out/\"+sub_array[i],overwrite=True) # move to the out directory\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c033a76",
   "metadata": {},
   "source": [
    "koy: (R) Alignment clearly did not work...fixed with plate solve worksheet! (U) It looks like the reference image did not match the scientific image for all the images. U is a bad choice for filter! Ok, after taking one out, they at least all look like they are pointed at the right thing. Let's try photometry!\n",
    "\n",
    "sse: Clearly didn't work - subtracted images look insane. I tried many, many, many things....but actually, I think they are fine! It's just that the saturated stars in the window never get subtracted right (and we should not expect them to!), so they are just always wrong. The background still looks around zero, and the non-saturated sources actually look ok. I think the saturation flag filter is GOOD in these cases, but I think we should move right on to (attempting...) photometry.\n",
    "\n",
    "umf: Same problem as sse, there are saturated stars in the subtraction region, making the subtractions look wrong. But, I think they are right, and if we want them to look better we've have to totally avoid the saturated stars in the reference, which would make the cutout like 20 x 20. One could run it that way one time to check, but the images actually kinda look fine, so let's try straight on with photometry.\n",
    "\n",
    "yfh: without saturation mask, and there are still saturated stars in these images. We might want to shrink the sizes of the windows, but let's try to mask the saturated stars first. Yeah that masking actually SCREWED UP the subtraction MORE...go back and change the windows I suppose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb567b",
   "metadata": {},
   "source": [
    "Let's plot them all to get a good look. And also to get an initial flux estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06e7d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fig, axs = plt.subplots(len(sci_conv_array), 2, figsize=(50,50)) # ugh figsize does nothing.\n",
    "for i in range(len(sci_conv_array)):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(50,50))\n",
    "    sci_image = fits.open(sci_conv_array[i])\n",
    "    sub_image = fits.open(\"out/\"+sub_array[i])\n",
    "    \n",
    "    w = WCS(sub_image[0].header)\n",
    "    #print(w)\n",
    "    coord = SkyCoord(ra=RA * u.deg, dec=Dec * u.deg, frame='icrs')\n",
    "    #print(coord)\n",
    "    x,y=w.world_to_pixel(coord)\n",
    "    #print(x,y)\n",
    "    \n",
    "    aper = CircularAperture((x,y), 8)\n",
    "    \n",
    "    mean, median, std = sigma_clipped_stats(sub_image[0].data)\n",
    "    axs[0].plot(aper)\n",
    "    #axs[0].scatter(x, y, marker='o', color='r', facecolors='none',s=1000,linewidths=2)\n",
    "    axs[0].imshow(sub_image[0].data, vmin=median-2*std, vmax=median+2*std, cmap='gray')\n",
    "        \n",
    "    mean, median, std = sigma_clipped_stats(sci_image[0].data)\n",
    "    axs[1].imshow(sci_image[0].data, vmin=median-2*std, vmax=median+2*std, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48144a1",
   "metadata": {},
   "source": [
    "Evaluating these images is kind of a nightmare, after the PSFing. We need to figure out a way to reduce the number of saturated stars...online calculator? Or is that my job...?\n",
    "\n",
    "This sheet actually might do that, it looks observatory independent?\n",
    "\n",
    "https://roman.gsfc.nasa.gov/science/ETC2/ExposureTimeCalc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac6050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
